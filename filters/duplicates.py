################################################################################
#                                                                              #
#   This file is part of the Bibolamazi Project.                               #
#   Copyright (C) 2013 by Philippe Faist                                       #
#   philippe.faist@bluewin.ch                                                  #
#                                                                              #
#   Bibolamazi is free software: you can redistribute it and/or modify         #
#   it under the terms of the GNU General Public License as published by       #
#   the Free Software Foundation, either version 3 of the License, or          #
#   (at your option) any later version.                                        #
#                                                                              #
#   Bibolamazi is distributed in the hope that it will be useful,              #
#   but WITHOUT ANY WARRANTY; without even the implied warranty of             #
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the              #
#   GNU General Public License for more details.                               #
#                                                                              #
#   You should have received a copy of the GNU General Public License          #
#   along with Bibolamazi.  If not, see <http://www.gnu.org/licenses/>.        #
#                                                                              #
################################################################################


import os
import os.path
import re
import codecs
import unicodedata


from pybtex.database import BibliographyData;


from core.bibfilter import BibFilter, BibFilterError;
from core.blogger import logger;
from core.latex import latex2text

import arxivutil

### DON'T CHANGE THIS STRING. IT IS THE STRING THAT IS SEARCHED FOR IN AN EXISTING
### DUPFILE TO PREVENT OVERWRITING OF WRONG FILES.
BIBALIAS_WARNING_HEADER = """\
% NOTE: THIS FILE WAS AUTOMATICALLY GENERATED BY bibolamazi SCRIPT!
%       ANY CHANGES WILL BE LOST!
"""

BIBALIAS_HEADER = ur"""
%
####BIBALIAS_WARNING_HEADER####
%
% You should include this file in your main LaTeX file with the command
%
%   \input{####DUP_FILE_NAME####}
%
% in your document preamble.
%



%
% The following will define the command \bibalias{<alias>}{<source>}, which will make
% the command \cite[..]{<alias>} the same as doing \cite[..]{<source>}.
%
% This code has been copied and adapted from
%    http://tex.stackexchange.com/questions/37233/
%

\makeatletter
% \bibalias{<alias>}{<source>} makes \cite{<alias>} equivalent to \cite{<source>}
\newcommand\bibalias[2]{%
  \@namedef{bibali@#1}{#2}%
}

\newtoks\biba@toks
\let\bibalias@oldcite\cite
\renewcommand\cite[2][]{%
  \biba@toks{\bibalias@oldcite#1}%
  \def\biba@comma{}%
  \def\biba@all{}%
  \@for\biba@one:=#2\do{%
    \@ifundefined{bibali@\biba@one}{%
      \edef\biba@all{\biba@all\biba@comma\biba@one}%
    }{%
      \PackageInfo{bibalias}{%
        Replacing citation `\biba@one' with `\@nameuse{bibali@\biba@one}'
      }%
      \edef\biba@all{\biba@all\biba@comma\@nameuse{bibali@\biba@one}}%
    }%
    \def\biba@comma{,}%
  }%
  \edef\biba@tmp{\the\biba@toks{\biba@all}}%
  \biba@tmp
}
\makeatother


%
% Now, declare all the alias keys.
%

""".replace('####BIBALIAS_WARNING_HEADER####\n', BIBALIAS_WARNING_HEADER)



DUPL_WARN_TOP = """

    DUPLICATE ENTRIES WARNING
    -------------------------

"""

DUPL_WARN_ENTRY = """\
    %(alias)-25s \tis a duplicate of  %(orig)s
"""

DUPL_WARN_BOTTOM = """
    -------------------------

"""




HELP_AUTHOR = u"""\
Duplicates filter by Philippe Faist, (C) 2013, GPL 3+
"""

HELP_DESC = u"""\
Filter that detects duplicate entries and produces rules to make one entry an alias of the other.
"""

HELP_TEXT = u"""
This filter works by writing a LaTeX file to a specified location (via the
`dupfile' option) which contains the commands needed to define the bibtex
aliases.

Note that the dupfile option is mandatory in order to create the file with
duplicate definitions. You need to specify a file to write to. You may do this
with `--dupfile=dupfile.tex' or with `-sDupfile=dupfile.tex'.

In your main LaTeX document, you need to add the following command in the
preamble:

  \input{yourdupfile.tex}

where of couse yourdupfile.tex is the file that you specified to this filter.

Alternatively, if you just set the warn flag on, then a duplicate file is not
created (unless the dupfile option is given), and a warning is displayed for
each duplicate found.
"""



class DuplicatesFilter(BibFilter):

    helpauthor = HELP_AUTHOR
    helpdescription = HELP_DESC
    helptext = HELP_TEXT


    def __init__(self, dupfile=None, warn=False):
        """DuplicatesFilter constructor.

        *dupfile: the name of a file to write latex code for defining duplicates to. This file
                  will be overwritten!!
        *warn: if this flag is set, dupfile is not mandatory, and a warning is issued for every
               duplicate entry found in the database.
        """

        BibFilter.__init__(self);

        self.dupfile = dupfile
        self.warn = warn

        if (not self.dupfile and not self.warn):
            logger.warning("bibolamazi duplicates filter: no action will be taken as neither -sDupfile or"+
                           " -dWarn are given!")

        logger.debug('duplicates: dupfile=%r, warn=%r' % (dupfile, warn));


    def name(self):
        return "duplicates"

    def getRunningMessage(self):
        if (self.dupfile):
            return (u"processing duplicate entries. Don't forget to insert `\\input{%s}' in "
                    "your LaTeX file!" %(self.dupfile) );
        return u"processing duplicate entries (warning will be generated only)"
    

    def action(self):
        return BibFilter.BIB_FILTER_BIBOLAMAZIFILE;


    def prepare_entry_cache(self, a, cache_a, arxivaccess):
        def getlast(pers):
            # join last names
            last = unicodedata.normalize('NFKD', delatex(" ".join(pers.prelast()+pers.last())).split()[-1].lower());
            # remove any unicode compositions (accents, etc.)
            last = re.sub(r'[^\x00-\x7f]', '', last.encode('utf-8')).decode('utf-8')
            ## additionally, remove any special LaTeX chars which may be written differently.
            #last = re.sub(r'\\([a-zA-Z]+|.)', '', last);
            last = re.sub(r'(\{|\})', '', last);

            initial = pers.first(True)
            return (last, initial);

        cache_a['pers'] = [ getlast(pers) for pers in a.persons.get('author',[]) ]

        cache_a['arxivinfo'] = arxivaccess.getArXivInfo(a.key)

        note = a.fields.get('note', '')
        cache_a['note_cleaned'] = (arxivutil.stripArXivInfoInNote(note) if note else "")
        
        cache_a['j_abbrev'] = re.sub('[^A-Z]', '', a.fields.get('journal', ''));

        def cleantitle(title):
            title = unicodedata.normalize('NFKD', unicode(delatex(title).lower()))
            # remove any unicode compositions (accents, etc.)
            title = re.sub(r'[^\x00-\x7f]', '', title.encode('utf-8')).decode('utf-8')
            # remove any unusual characters
            title = re.sub(r'[^a-zA-Z0-9 ]', '', title)
            # remove any inline math
            title = re.sub(r'$[^$]+$', '', title)
            # clean up whitespace
            title = re.sub(r'\s+', ' ', title)
            return title.strip()

        cache_a['title_clean'] = cleantitle(a.fields.get('title', ''))


    def compare_entries_same(self, a, b, cache_a, cache_b):

        # compare author list first

        apers = cache_a['pers']
        bpers = cache_b['pers']

        if (len(apers) != len(bpers)):
            return False

        for k in range(len(apers)):
            (lasta, ina) = apers[k]
            (lastb, inb) = bpers[k]
            if (lasta != lastb or (ina and inb and ina != inb)):
                #print "Authors %r and %r differ" %((lasta, ina), (lastb, inb))
                return False

        #print "Author list matches! %r and %r "%(apers,bpers);

        def compare_neq_fld(x, y, fld, filt=lambda x: x):
            xval = x.get(fld, None);
            yval = y.get(fld, None);
            try:
                xval = xval.strip();
            except AttributeError:
                pass
            try:
                yval = yval.strip();
            except AttributeError:
                pass

            return xval is not None and yval is not None and filt(xval) != filt(yval) ;

        # authors are the same. check year
        if (compare_neq_fld(a.fields, b.fields, 'year')):
            ##print "years differ!"
            return False

        if (compare_neq_fld(a.fields, b.fields, 'month')):
            #print "months differ!"
            return False

        doi_a = a.fields.get('doi')
        doi_b = b.fields.get('doi')
        if (doi_a and doi_b and doi_a != doi_b):
            #print "doi differs!"
            return False
        if (doi_a and doi_a == doi_b):
            logger.debug("entries have same doi: %r, %r" %(a.key, b.key))
            return True

        arxiv_a = cache_a['arxivinfo']
        arxiv_b = cache_b['arxivinfo']

        #print "arxiv_a=%r, arxiv_b=%r" %(arxiv_a, arxiv_b)
        
        if (arxiv_a and arxiv_b and
            'arxivid' in arxiv_a and 'arxivid' in arxiv_b and
            arxiv_a['arxivid'] != arxiv_b['arxivid']):
            #print "arxivid differs!"
            return False
        if (arxiv_a and arxiv_b and
            'arxivid' in arxiv_a and 'arxivid' in arxiv_b and
            arxiv_a['arxivid'] == arxiv_b['arxivid']):
            #print "same arxivid!"
            logger.debug("entries have same arXiv id: %r, %r" %(a.key, b.key))
            return True


        # if they have different notes, then they're different entries
        note_cl_a = cache_a['note_cleaned']
        note_cl_b = cache_b['note_cleaned']
        if (note_cl_a and note_cl_b and note_cl_a != note_cl_b):
            return False

        # create abbreviations of the journals by keeping only the uppercase letters
        j_abbrev_a = cache_a['j_abbrev']
        j_abbrev_b = cache_b['j_abbrev']
        if (j_abbrev_a and j_abbrev_b and j_abbrev_a != j_abbrev_b):
            #print "journals differ!"
            return False

        if ( compare_neq_fld(a.fields, b.fields, 'volume') ):
            #print "volume differ!"
            return False

        if ( compare_neq_fld(a.fields, b.fields, 'number') ):
            #print "number differ!"
            return False

        titlea = cache_a['title_clean']
        titleb = cache_b['title_clean']

        if (titlea and titleb and titlea != titleb):
            logger.debug("Titles of entries %r and %r differ." %(a.key, b.key))
            return False

        # ### Unreliable. Bad for arxiv entries and had some other bugs. (E.g. "123--5" vs "123--125" vs "123")
        #
        #if ( compare_neq_fld(a.fields, b.fields, 'pages') ):
        #    print "pages differ!"
        #    import pdb; pdb.set_trace()
        #    return False

        #print "entries match!"

        # well at this point the publications are pretty much duplicates
        return True
        


    def filter_bibolamazifile(self, bibolamazifile):
        #
        # bibdata is a pybtex.database.BibliographyData object
        #

        bibdata = bibolamazifile.bibliographydata();

        duplicates = [];

        newbibdata = BibliographyData();

        arxivaccess = arxivutil.get_arxiv_cache_access(bibolamazifile)

        # In a future version, we could imagine using the bibolamazi cache, and not recalculating
        # these values if they are already in the cache. However:
        #
        # NOTE: It is important that this cache is UP TO DATE, because otherwise if the user notices
        #       that two entries are matched falsely as duplicates and modifies one of the entries,
        #       it has to be picked up in the cache!
        #
        # So the simplest is to always recalculate the cache for all entries. It's fast in practice.
        # We actually don't need to store it in the bibolamazi cache.
        #
        #cache_entries = self.cache_for('duplicates_entryinfo_cache')
        cache_entries = {};

        for (key, entry) in bibdata.entries.iteritems():
            cache_entries[key] = {}
            self.prepare_entry_cache(entry, cache_entries[key], arxivaccess)

        for (key, entry) in bibdata.entries.iteritems():
            #
            # search the newbibdata object, in case this entry already exists.
            #
            logger.longdebug('inspecting new entry %s ...', key);
            is_duplicate_of = None
            for (nkey, nentry) in newbibdata.entries.iteritems():
                if self.compare_entries_same(entry, nentry, cache_entries[key], cache_entries[nkey]):
                    logger.longdebug('    ... matches existing entry %s!', nkey);
                    is_duplicate_of = nkey;
                    break

            #
            # if it's a duplicate
            #
            if is_duplicate_of is not None:
                dup = (key, is_duplicate_of)
                duplicates.append(dup);
            else:
                newbibdata.add_entry(key, entry);

        # output duplicates to the duplicates file

        if (self.dupfile):
            ### TODO: do a minimum checks before overwriting:
            ###       * has a previously-written dupfile header
            ###       * compare modif. time w/ some reference?
            ###       * add --force-overwrite flag?
            dupfilepath = os.path.join(bibolamazifile.fdir(),self.dupfile);
            check_overwrite_dupfile(dupfilepath);
            dupstrlist = [];
            with codecs.open(dupfilepath, 'w', 'utf-8') as dupf:
                dupf.write(BIBALIAS_HEADER.replace('####DUP_FILE_NAME####', self.dupfile));
                for (dupalias, duporiginal) in duplicates:
                    dupf.write((r'\bibalias{%s}{%s}' % (dupalias, duporiginal)) + "\n");
                    dupstrlist.append("\t%s is an alias of %s" % (dupalias,duporiginal)) ;

                dupf.write('\n\n');

            # issue debug message
            logger.debug("wrote duplicates to file: \n" + "\n".join(dupstrlist));

        if (self.warn):
            logger.warning(DUPL_WARN_TOP  +
                           "".join([ DUPL_WARN_ENTRY % { 'alias': dupalias,
                                                         'orig': duporiginal
                                                         }
                                     for (dupalias, duporiginal) in duplicates
                                     ])  +
                           DUPL_WARN_BOTTOM);


        bibolamazifile.setBibliographyData(newbibdata);

        return


def bibolamazi_filter_class():
    return DuplicatesFilter;





def delatex(s):
    # Fixed: bug in pybtex.
    #    ### FIXME: Where the hell are all the "\~"'s being replaced by "\ " ??
    #    s = s.replace(r'\ ', r'\~');
    return latex2text.latex2text(unicode(s));


def check_overwrite_dupfile(dupfilepath):
    if (not os.path.exists(dupfilepath)):
        return
    # path item exists (but could be dir, etc.)
    if (not os.path.isfile(dupfilepath)):
        raise BibFilterError('duplicates', "Can't overwrite non-file path `%s'"% (dupfilepath))
    
    with codecs.open(dupfilepath, 'r') as f:
        head_content = u'';
        for countline in xrange(10):
            head_content += f.readline()

    if BIBALIAS_WARNING_HEADER not in head_content:
        raise BibFilterError('duplicates', "File `%s' does not seem to have been generated by bibolamazi. Won't overwrite. Please remove the file manually." %(dupfilepath))
